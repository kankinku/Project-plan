### 1. Role Refs의 **"VLM을 통한 이미지 인식"** 을 개선하기로 확정했기 때문에 VLM을 활용했을때, 어느정도의 토큰 소모가 되는지 확인해본다.
	- sonnet 4.5 기준 512x512 픽셀당 800토큰이 사용된다.
	- !새롭게 알게된 사실 = Opus 4.5의 경우 sonnet4.5동일한 토큰이 사용된다. 
		[[claude-bills]]
-> 기존의 시스템에서는 웹페이지 한번의 1920×1080이고, 찾아본 코드에서는 따로 리사이징 후 전송이 없기 때문에 **"총 12타일"** 9600토큰이 소모된다. (어디까지나 예측의 영역입니다.)

![[Pasted image 20260131203638.png]]

+출력과 프롬프트에 관련된 토큰은 추가로 소모됩니다.

---

### 2. OCR-First + DOM Fallback + Micro-VLM
-> 시스템에서 페이지를 이동할때마다, 한번의 액션이라고 가정했을때, 사용되는 토큰의 양이 엄청날 것이라고 예상된다.

1) 대안1 : 초경량 OCR을 통해서 이미지 인식
	- 예를 들어서 PaddleOCR를 활용 (https://github.com/PaddlePaddle/PaddleOCR/blob/main/readme/README_ko.md)
	- 만약 텍스트와 이미지 인식의 실패할 경우를 대비하여 풀백으로 DOM으로 처리하는 과정을 남겨둔다.
2) 대안2 : 부분적 Micro-VLM
	- 로그인 버튼의 위치는 사이드에, 검색을 상단에, 이런 일반적인 부분을 우선 탐색하는 탐색 방식을 도입한다.
	- 구체적인 방법에 대해서 아직 미정

## 3. 얼마나 세이브 가능한가.
-> 이미지 인식에서 사용되는 토큰이 로컬에서 진행되기 때문에, 그 부분을 세이브 가능하다.

| 즉 페이지당 9600토큰 정도의 토큰 낭비가 세이브된다.

---

## 4. 새로운 방식에 대한 탐구 : "Visual Grounding" 도입

현재 고려하시는 대안들을 하나로 묶는 핵심 개념은 **Visual Grounding**입니다.

Microsoft의 **OmniParser** 같은 구조를 참고해 보려고 한다. 이는 로컬에서 UI 엘리먼트를 박스로 감지(Detection)하고 아이콘 의미를 태깅한 뒤, LLM에게는 "어디에 무엇이 있다"라는 **'태그가 달린 스크린샷'** 이나 **'좌표 리스트'** 만 전달하는 방식이다.

웹 UI 전용으로 학습된 가벼운 로컬 모델(예: **ScreenAI**, **SeeClick** 관련 경량 모델)을 사용하는 방법에 대해서도 테스트를 진행하여 성공률에 대한 지표를 알아볼 필요성을 느낀다.
